{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3632c018-d33d-4aa4-8507-ff27be149ebb",
   "metadata": {},
   "source": [
    "Question 1: By using both a coding and a non-coding-based approaches, scrape a total of 75 pins\n",
    "from the Pinterest about something you are passionate. Make sure your scraped data contains both\n",
    "images and users’ reactions (e.g., number of shares, comments, repins) to images (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbd9b4-3ce0-42e3-b4d4-6ad0c6875bad",
   "metadata": {},
   "source": [
    "I worked specifically on food images and scraped 66 image URLs using a Python-based Pinterest scraper. In addition to that, I used Apify — a no-code platform — to scrape more images for the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371490f-70c4-4dcd-bd44-5226005885cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "# List of topics to scrape\n",
    "topics = [\n",
    "    \"traditional indian food\",\n",
    "    \"south indian food\",\n",
    "    \"north indian thali\",\n",
    "    \"indian sweets\",\n",
    "    \"street food india\",\n",
    "    \"homemade indian dishes\"\n",
    "]\n",
    "\n",
    "# Scroll depth and image limit per celebrity\n",
    "scroll_depth = 35\n",
    "image_target = 20\n",
    "\n",
    "# Bluetooth bug circumnavigate (keep existing options)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "\n",
    "# Initialize Selenium driver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Initialize a list to store results\n",
    "all_output = []\n",
    "\n",
    "# Loop over each celebrity to scrape data\n",
    "for topic in topics:\n",
    "    print(f\"Scraping images for: {topic}\")\n",
    "\n",
    "    # Create the Pinterest search URL for the current celebrity\n",
    "    query = topic.replace(\" \", \"%20\")\n",
    "    url = \"https://pinterest.com/search/pins/?q=\" + query\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll the page to load more pins\n",
    "    for _ in range(1, scroll_depth):\n",
    "        driver.execute_script(\"window.scrollTo(1,100000)\")\n",
    "        print(f\"Scrolling for {topic}...\")\n",
    "        sleep(randint(1, 4))\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    results = soup.find_all(\"a\")\n",
    "\n",
    "    # Extract URLs for the pins\n",
    "    urls = []\n",
    "    for ele in results:\n",
    "        href = ele.get('href')\n",
    "        if \"/pin/\" in href:\n",
    "            urls.append(f\"https://www.pinterest.com{href}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    urls = list(set(urls))\n",
    "\n",
    "    # Spoof user-agent to avoid being blocked\n",
    "    ua = UserAgent()\n",
    "    headers = {'User-Agent': str(ua.chrome)}\n",
    "\n",
    "    # Track the number of images scraped for the current celebrity\n",
    "    topic_output = []\n",
    "\n",
    "    for ind, url in enumerate(urls):\n",
    "        if len(topic_output) >= image_target:  # Stop when images target is met\n",
    "            break\n",
    "\n",
    "        print(f\"Scraping URL for {topic}: {url}\")\n",
    "        # Scrape page with spoofed user-agent\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the image URL from the page\n",
    "        img = soup.find(\"img\", class_=\"hCL kVc L4E MIw\")\n",
    "        img_url = img.get(\"src\") if img else None\n",
    "\n",
    "        all_scripts = soup.find_all(\"script\")\n",
    "        data = {}\n",
    "\n",
    "        # Extract Pinterest data from the scripts\n",
    "        for script in all_scripts:\n",
    "            if \"requestParameters\" in script.text:\n",
    "                try:\n",
    "                    raw_json = json.loads(script.get_text())\n",
    "                    request_param = raw_json[\"requestParameters\"]\n",
    "                    if request_param[\"name\"] == \"CloseupDetailQuery\":\n",
    "                        response = raw_json[\"response\"]\n",
    "                        data_raw = response[\"data\"]\n",
    "                        pin_query = data_raw[\"v3GetPinQuery\"]\n",
    "                        data = pin_query[\"data\"]\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        # Initialize a dictionary to store pin data\n",
    "        pin_dict = {\n",
    "            \"topic\": topic,\n",
    "            \"url\": url,\n",
    "            \"img_url\": img_url,\n",
    "        }\n",
    "\n",
    "        if data:\n",
    "            # Extract details from the scraped data\n",
    "            pin_dict[\"title\"] = data.get(\"gridTitle\", \"\")\n",
    "            pin_dict[\"description\"] = data.get(\"closeupUnifiedDescription\", \"\")\n",
    "            pin_dict[\"share_count\"] = data.get(\"shareCount\", 0)\n",
    "            pin_dict[\"repin_count\"] = data.get(\"repinCount\", 0)\n",
    "\n",
    "            # Extract poster data\n",
    "            poster_raw = data.get(\"pinner\", {})\n",
    "            pin_dict[\"poster_username\"] = poster_raw.get(\"username\", \"\")\n",
    "            pin_dict[\"poster_followers\"] = poster_raw.get(\"followerCount\", 0)\n",
    "\n",
    "            # Extract comment and reaction data\n",
    "            agg_pin_data = data.get(\"aggregatedPinData\", {})\n",
    "            pin_dict[\"comment_count\"] = agg_pin_data.get(\"commentCount\", 0)\n",
    "\n",
    "            # Reactions\n",
    "            reacts_raw = data.get(\"reactionCountsData\", [])\n",
    "            reactions = 0\n",
    "            for r in reacts_raw:\n",
    "                key_str = f\"react_type_{r['reactionType']}\"\n",
    "                pin_dict[key_str] = r[\"reactionCount\"]\n",
    "                reactions += r[\"reactionCount\"]\n",
    "            pin_dict[\"reactions\"] = reactions\n",
    "\n",
    "        # Add the image data to the celeb_output list\n",
    "        topic_output.append(pin_dict)\n",
    "\n",
    "        # Random wait to avoid getting blocked\n",
    "        sleep(randint(5, 30))\n",
    "\n",
    "    # Add the current celebrity's output to the main output list\n",
    "    all_output.extend(topic_output)\n",
    "\n",
    "# Convert the results to a DataFrame and save to CSV\n",
    "pin_df = pd.DataFrame(all_output)\n",
    "pin_df.to_csv('TIF_Results.csv', index=True, index_label=\"Index\")\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
